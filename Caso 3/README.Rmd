---
title: "README"
author: "Diego Rodríguez Barreda"
date: "15 de junio de 2016"
output: html_document
---

#Benemérita Universidad Autónoma de Puebla
#Facultad de Ciencias Físico Matemáticas

##Licenciatura en Actuaria

##Programación Actuarial III

###Caso: Reconocimiento de Actividad Humana con Celulares

El propósito de este proyecto es demostrar tu habilidad para recolectar, trabajar y limpiar base de datos. El objetivo es preparar un conjunto ordenado de información que pueda ser trabajado en análisis posteriores.

Una descripción de la base de datos y su forma de creación puede ser encontrada en la siguiente liga: >http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones>

Los datos utilizados en este proyecto pueden ser descargados en la siguiente liga:
<https://www.dropbox.com/s/j26ksrw1jzqz2a1/getdata-projectfilesUCI%20HAR%20Dataset.zip?dl=0>


Este proyecto contiene los siguientes archivos:


**README.md**: Donde se hace una descripción de los pasos para hacer el análisis.

**CodeBook.md**: Donde aparecen las variables, la base de datos y las transformaciones que se realizaron a la misma.

**correr_analisis.R**: Es el código que ejecuta el análisis.

**BaseCompleta.txt**: Base de datos ya ordenada con los 30 sujetos y los 66 datos.

Para realizar el análisis de la base de datos es necesario ejecutar el archivo coreer_analisis.R y correr el código.



#####El análisis hará lo siguiente:

-Primero une los datos de los archivos test con los de los archivos training, para crear un solo conjunto de datos.

-Después extrae las medidas de media y desviación estándar de cada medición.

-Luego usa nombres de actividad correspondientes para describir los nombres de las actividades en la base de datos. 

-Coloca los nombres apropiados en la base de datos con etiquetas de variables que las describan.

-Se crea una segunda base de datos con el promedio de cada variable para cada actividad y cada sujeto.


####Funcionamiento del código

El código **correr_analisis.R** realiza lo siguiente:



Une los datos de los archivos test con los de los archivos training, para crear un solo conjunto de datos

Primero entramos a la dirección que utilizaremos para trabajar

 setwd("~/programaci-n_Actuarial_III/Caso 3/UCI HAR Dataset")
 
 
Luego abrimos cada archivo necesario de test y train:


`xtrain <-read.table("./train/X_train.txt")`

`ytrain <-read.table("./train/Y_train.txt")`

`strain <-read.table("./train/subject_train.txt")`

`xtest <- read.table("./test/X_test.txt")`

`ytest <- read.table("./test/y_test.txt")`

`stest <- read.table("./test/subject_test.txt")`


    
Después combinamos los archivos apropiadamente y removemos lo que no utilizaremos más:

`combX <- rbind(xtrain, xtest)` 
`combY <- rbind(ytrain, ytest)`
`combS <- rbind(strain, stest)`


`rm(xtrain)`
`rm(ytrain)`
`rm(strain)`
`rm(xtest)`
`rm(ytest)`
`rm(stest)`

Extraemos las medidas de media y desviación estándar de cada medición del archivo **features.txt** quitando lo innecesario de cada dato:

`caract <- read.table("./features.txt")`

`promedioStdIndex <- grep("mean\\(\\)|std\\(\\)", caract[, 2])`

`combX <- combX[, promedioStdIndex]`
`names(combX) <- gsub("\\(\\)", "", caract[promedioStdIndex, 2])`
`names(combX) <- gsub("mean", "Mean", names(combX)) `
`names(combX) <- gsub("std", "Std", names(combX))` 
`names(combX) <- gsub("-", "", names(combX)) `

Y removemos de la memoria de nuevo lo que ya no utizaremos:

`rm(caract)`
`rm(promedioStdIndex)`

Usamos ahora el nombre de la actividad para llamar las actividades en la nueva base de datos, utilizando el archivo **activity_labels.txt**:

`activity <- read.table("./activity_labels.txt")`
`activity[, 2] <- tolower(gsub("_", "", activity[, 2])) `
`substr(activity[2, 2], 8, 8) <- toupper(substr(activity[2, 2], 8, 8)) `
`substr(activity[3, 2], 8, 8) <- toupper(substr(activity[3, 2], 8, 8)) `
`combY[, 1] <- activity[combY[, 1], 2]`
`names(combY) <- "actividad"`

Damos el nombre apropiado en las etiquetas para la nueva base de datos combinando las etiquetas de cada dirección y removemos lo que ya no utilizaremos:

`names(combS) <- "sujeto"`
`datoslimpios <- cbind(combS, combY, combX)`
`rm(combX)`
`rm(combY)`


Creamos una segunda base de datos a partir de la ya recopilada pero con el promedio de cada medición para reducir los datos y tener la Base de datos completa requerida:

`SLen <- length(table(combS)) `
`activityLen <- dim(activity)[1]`
`colLen <- dim(datoslimpios)[2]`
`resultado <- as.data.frame(matrix(NA, nrow=SLen*activityLen, ncol=colLen))`
`colnames(resultado) <- colnames(datoslimpios)`
`d <- 1`
`for(i in 1:SLen) {`
    `for(k in 1:activityLen) {`
        `resultado[d, 1] <- sort(unique(combS)[, 1])[i]`
        `resultado[d, 2] <- activity[k, 2]`
        `die1 <- i == datoslimpios$sujeto`
        `die2 <- activity[k, 2] == datoslimpios$actividad`
        `resultado[d, 3:colLen] <- colMeans(datoslimpios[die1&die2, 3:colLen])`
        `d <- d + 1`
    `}`
`}`

Y por último creamos nuestro archivo de texto con la nueva base de datos ya ordenada y con lis promedios de cada una de las 66 mediciones y de los 30 sujetos en cada una de sus 6 actividades:

`write.table(resultado, "BaseCompleta.txt", row.name=FALSE)`







